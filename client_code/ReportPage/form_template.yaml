components:
- components:
  - event_bindings: {click: btn_back_click}
    layout_properties: {grid_position: 'XHFNXA,DBSDHK'}
    name: button_back
    properties: {text: <-- Back TO GAME}
    type: Button
  - layout_properties: {grid_position: 'BBTUSR,YXVWOZ'}
    name: rich_text_2
    properties: {content: '# **Blog Report**'}
    type: RichText
  - layout_properties: {grid_position: 'DCNRYM,ZTUFAV'}
    name: rich_text_1
    properties:
      content: "# Crafting a High-Quality Connect 4 Dataset\nTo effectively train an AI to play Connect 4, a robust and extensive dataset of game states and corresponding optimal moves is crucial. Our approach focuses on generating such a dataset through advanced game simulation, rather than relying on human gameplay or simpler heuristics.\n\n## Core Methodology: Monte Carlo Tree Search (MCTS)\nThe heart of our dataset generation lies in simulating a large number of Connect 4 games, with each move meticulously chosen by Monte Carlo Tree Search (MCTS). Key settings for our simulation include:\n* **Total Games Simulated**: 5,000 self-play games\n* **MCTS Decision Depth**: For each move, the MCTS performs 1,000 simulation steps\n* **Data Augmentation**: Horizontal mirroring was used for doubling the effective dataset size\n* **Board encoding**: Each Connect 4 board state is represented as a 6x7 grid of +1s, -1s, and 0s. \n    * **0**: Represents an empty slot on the board.\n    * **1**: Represents a piece belonging to the 'plus' player.\n    * **-1**: Represents a piece belonging to the 'minus' player.\n\n## The Generation Process\nEach game begins with an empty board. While the initial few moves might be randomized to encourage variety in early game states, the MCTS quickly takes over. As the MCTS determines the best move for a given board state, that state and the chosen move are recorded. This process continues until a game concludes with a winner or a draw.\n\n### Final Dataset Post-Processing\nOnce all games are simulated and initial data collected, a final step refines the dataset:\n* **Deduplication with Majority Vote**: All instances of a unique board state are grouped, and the final 'best move' assigned to it is determined by a majority vote among all recorded moves for that state. This creates a clean and consistent dataset despite the stochastic nature of MCTS.\n\n### Dataset Output\n* **Raw augmented samples**: 253,248 boards\n* **After deduplication**: 200,597 unique board states\n* **Features (X)**: Numerical representations of the board states, shape (200597, 6, 7)\n* **Labels (Y)**: The optimal column (move) to play for each board state\n\n---\n\n# Creating the CNN and What It Told Us\n\n### Approach\nWe built the CNN with a simple goal: give the model a consistent view of the game and then let it learn patterns at multiple levels. The board is converted into a 6×7×2 tensor, where one channel is the current player’s pieces and the other is the opponent’s pieces. This perspective flip happens during preprocessing, which keeps the input consistent even when turns switch.\n\n### Architecture\nWe started with a standard convolution layer and then used several **residual blocks**. The skip connections help train a deeper CNN without gradients fading out. We increased the number of filters as we went deeper (**64 → 128 → 256**), letting the model learn higher‑level patterns like blocking threats. Instead of flattening, we used **global average pooling** to reduce overfitting.\n\n### Training Setup\nWe used two dense layers with dropout (0.4) and a final softmax over the 7 columns. Training utilized **Adam** with a cosine decay learning rate schedule and early stopping.\n\n### Results and Interpretation\n* **Top‑1 Accuracy**: 61.68%\n* **Top‑2 Accuracy**: 80.72%\nThe model learns useful policy signals, but we observed some overfitting (validation stayed around ~0.59). \n\n---\n\n# Building the Transformer Model and What It Showed Us\n\n### Why Try a Transformer?\nWhile CNNs learn spatial structure through local filters, Transformers rely on **self-attention** to model relationships between all positions simultaneously. In Connect 4, threats are not purely local; a Transformer can evaluate global board interactions in every layer.\n\n### Architecture\n* Flattened the 6×7 grid into a sequence of **42 positions**.\n* **Embedding layer**: Mapped board cells into a higher-dimensional space.\n* **Encoder blocks**: Multi-head self-attention, feedforward layers, and layer normalization.\n* **Global pooling**: Across the 42 tokens.\n\n### Results and Interpretation\n* **Unmasked Top-1 Accuracy**: 62.10%\n* **Masked Top-2 Accuracy**: 81.32%\nThe model rarely predicts illegal moves, showing it internalized basic game constraints. Performance is slightly improved over the CNN, but the modest gap suggests that Connect 4’s spatial structure may already be captured efficiently by convolutional layers.\n\n# What This Comparison Suggests\nAt this scale (200k unique states), neither architecture has a dramatic advantage. The CNN excels at local spatial patterns, while the Transformer models global interactions. Future experiments might explore hybrid CNN-Transformer architectures to combine these strengths."
    type: RichText
  layout_properties: {slot: default}
  name: content_panel
  properties: {}
  type: ColumnPanel
- components:
  - layout_properties: {}
    name: headline_2
    properties: {role: headline, text: 'Hardy Smith, Kumar Rishu, Pranit Yadav, Connor Flaherty'}
    type: Label
  layout_properties: {slot: nav-right}
  name: navbar_links
  properties: {}
  type: FlowPanel
- layout_properties: {slot: title}
  name: headline_1
  properties: {role: headline, text: Report Page}
  type: Label
container:
  properties: {html: '@theme:standard-page.html'}
  type: HtmlTemplate
is_package: true
